
[toc]

# 决策树模型

决策树基于“树”结构进行决策
- 每个“内部节点”对应某个属性上的一个测试（test）
- 每个分支对应于该测试的一种可能结果（即该属性的某个取指）
- 每个“叶结点”对应于一个“预测结果“

学习过程：通过对训练样本的分析来确定”划分属性”*即内部节点所对应的属性）

预测结果：将测试示例从根节点开始，沿着划分属性所构成的“判定测试序列”下行，直到叶结点


# 建树流程

### 策略
- 自根至叶的递归过程
- 在每个中间结点寻找一个“划分”（split or test） 属性

### 三种停止条件
- 当前结点包含的样本全属于同一类别，无需划分；
- 当前属性集空，或是所有样本在所有属性上取值相同，无法划分；
- 当前结点包含的样本集合为空，不能划


## 基本算法

**输入**:  
训练集 $D = \{(x_1, y_1), (x_2, y_2), \dots, (x_m, y_m)\}$;  
属性集 $A = \{a_1, a_2, \dots, a_d\}$

**过程**:  
函数 `TreeGenerate(D, A)`

1. 生成结点 `node`;
2. **if** $D$ 中样本全属于同一类别 $C$ **then**
    - 将 `node` 标记为 $C$ 类叶结点; `return`
3. **end if**
4. **if** $A = \emptyset$ **OR** $D$ 中样本在 $A$ 上取值相同 **then**
    - 将 `node` 标记为叶结点，其类别标记为 $D$ 中样本数最多的类; `return`
5. **end if**
6. 从 $A$ 中选择最优划分属性 $a_*$;
7. **for** $a_*$ 的每一个值 $a_*^v$ **do**
    - 为 `node` 生成一个分支; 令 $D_v$ 表示 $D$ 中在 $a_*$ 上取值为 $a_*^v$ 的样本子集;
8.   **if** $D_v$ 为空 **then**
        - 将分支结点标记为叶结点，其类别标记为 $D$ 中样本最多的类; `return`
9.   **else**
        - 以 `TreeGenerate(D_v, A \setminus \{a_*\})` 为分支结点;
10. **end if**
11. **end for**

**输出**:  
以 `node` 为根结点的一棵决策树


## 划分依据

### 信息熵

信息熵（entropy）是度量样本集合“纯度”最常用的一种指标。

假定当前样本集合 $D$ 中第 $k$ 类样本所占的比例为 $p_k$，则 $D$ 的信息熵定义为

$$
\text{Ent}(D) = -\sum_{k=1}^{|\mathcal{Y}|} p_k \log_2 p_k
$$

计算信息熵时约定：若 $p = 0$，则 $p \log_2 p = 0$。

- $\text{Ent}(D)$ 的最小值为 0，最大值为 $ \log_2 |\mathcal{Y}| $。
- $\text{Ent}(D)$ 的值越小，则 $ D $ 的纯度越高。

信息增益直接以信息熵为基础，计算当前划分对信息熵所造成的变化。


### 信息增益



对于属性集 $a = \{a_1, a_2, \dots, a_d\}$，$D_v$为$D$在$a$上取指为$a_v$的样本集合

以属性$a$对数据集$D$进行划分所得到的信息增益为：
$$Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^V|}{|D|} Ent(D^V)$$


将所有可选属性的信息增益求出，选取信息增益最大的当前节点的划分属性

### 增益率

信息增益的缺点：对可取值数目较多的属性有所偏好，存在明显弱点，例如当id作为一个属性时，其信息增益率会很大

$$Gain\_ration(D,a) = \frac{Gain(D,a)}{-\sum_{v=1}^V \frac{|D^V|}{|D|} log_2 \frac{|D^V|}{|D|}}$$

此时，当属性$a$的可能取指越多（即V越大），则分母的值通常就越大

启发式：先从候选划分属性中找出信息增益高于平均水平的，在从中选取增益率最高的

### 基尼指数

对于一个数据集  D ，假设共有  K  类，记第  k  类样本所占的比例为  $p_k$ 。则数据集  D  的基尼指数定义为：

$$
\text{Gini}(D) = 1 - \sum_{k=1}^K p_k^2
$$

基尼指数衡量了数据集  D  中任意两样本属于不同类别的概率。它的取值范围是 \[0, 0.5]，值越小表示数据集的纯度越高。


对于属性集 $a = \{a_1, a_2, \dots, a_d\}$，$D_v$为$D$在$a$上取指为$a_v$的样本集合，属性$a_V$的基尼指数：

$$Gini\_index(D,a_V) = \sum_{v=1}^V \frac{|D^V|}{|D|} Gini(D^V)$$

在候选属性集合中，选取那个使划分后基尼指数最小的属性


# 剪枝

研究表明：划分选择的各种准则虽然对决策树的尺寸有较大影响，但对泛化性能的影响很有限例如信息增益与基尼指数产生的结果


剪枝方法和程度对决策树泛化性能的影响更显著



预剪枝和后剪枝是两种用于防止模型过拟合的方法。它们的目的都是减少决策树的复杂度，但实现方式和时机不同。以下是预剪枝和后剪枝的对比。

### 预剪枝：提前终止某些分支的生长

在树生成过程中，对于每个节点的分裂，先进行评估，如果分裂不能显著提高模型的效果，则停止分裂，即进行剪枝。例如，基于以下一些条件：
- 分裂前后的精度
- 分裂前后的信息增益
- ...


### 后剪枝：生成一颗完全树，在“回头”剪枝

生成完整决策树后，通过验证集评估节点的剪枝效果，移除对模型效果贡献不大的分支。具体步骤包括：
- 从树的最底层(叶子节点)开始，逐步剪掉节点，如果剪掉节点后模型在验证集上的准确性不降低，则保留该剪枝操作。
- 重复这一过程，直到进一步剪枝会降低模型的准确性。


### 对比

时间开销
- 预剪枝：训练时间开销降低，测试时间降低
- 后剪枝：训练时间开销增加，测试时间开销降低

过/欠拟合风险
- 预剪枝：过拟合风险降低，欠拟合风险增加
- 后剪枝：过拟合风险降低，欠拟合风向基本不变


泛化能力
- 后剪枝 通常优于 预剪枝

# 特殊情况

## 连续值

基本思路：连续属性离散化

常见做法：二分法

在机器学习的决策树算法中，二分法用于选择分割点来处理连续值特征，这个过程叫做“连续值的离散化”或“分裂点选择”。具体步骤如下：

1. **排序数据**  
   首先对连续值特征进行排序。假设特征属性是 $x$，有 $n$ 个样本，排序后得到：
   $$x_1 \leq x_2 \leq \dots \leq x_n$$

2. **计算候选分割点**  
   对于相邻的数值 $x_i$ 和 $x_{i+1}$，计算它们的中点作为候选分割点 $s$：
   $$s=\frac{x_i+x_{i+1}}{2}$$
   这样会产生 $n-1$ 个候选分割点。

3. **选择最佳分割点**  
   对每个候选分割点，将样本划分为左右两部分，然后计算分裂后的指标（如信息增益、信息增益率或基尼指数）。选择最优的分割点作为最终分割点。

4. **递归分裂**  
   找到最优分割点后，可以递归地对数据进行划分，直到满足停止条件（如达到最大深度或没有更多可分割的特征）。



## 缺失值

缺失现象：部分记录在指定属性的取指为空

基本思想：样本赋值，权重划分



对于存在缺失值的信息增益计算：

$$Gain(D,a) = Ent(D) - \sum_{v=1}^V \frac{|D^V|}{|D|} Ent(D^V)$$

在此之前的计算只需要关注不存在缺失值的记录

$$Gain(d,a) = \frac{缺失项数目}{总体数目} \cdot Gain(D,a)$$
## 多变量决策树

每个非叶节点不仅考虑一个属性
